{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "IBEX_Bayes_con_DQN_2_15ENE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m59",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcHPRRjl0LyS",
        "outputId": "4efa3563-68a9-476c-f446-c9bec24190f2"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print('Versión de TensorFlow: ' + tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Versión de TensorFlow: 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "If you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08e347c-d9be-4149-aff1-bff7a096c2d9"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install gym\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install pyglet\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents\n",
        "!pip install tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (650 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 145483 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting imageio==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/64/8e2bb6aac43d6ed7c2d9514320b43d5e80c00f150ee2b9408aee24359e6d/imageio-2.4.0.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (7.0.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-cp36-none-any.whl size=3303879 sha256=c2212f08f4a3e579d5e01d3b7c7d84373f83ffd0bcce37f532291cd66194747b\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/83/88/a1cba54ac06395d9e4ddcd9cf06911cd0b26cd78af9a61071b\n",
            "Successfully built imageio\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "Successfully installed imageio-2.4.0\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet) (0.16.0)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n",
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/cd/a0710b1caae042b7a4d54fc74073fb4df7adf073934798443bdc0059813a/tf_agents-0.7.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.12.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf-agents) (51.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.7.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (51.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NspmzG4nP3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ceafe223-63e6-4fc9-bc59-2de3a27f6b49"
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNm3TcaSH6R7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419d4aba-55d9-4df9-b841-7ec3fbdf9cc7"
      },
      "source": [
        "!pip install pandas-datareader\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (1.1.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (4.2.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->pandas-datareader) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->pandas-datareader) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->pandas-datareader) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas-datareader) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBF9kxBkH6R7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4978eb94-d35d-4e56-8e3c-4e43835ee7bb"
      },
      "source": [
        "!pip install pip yfinance --upgrade --no-cache-dir"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/eb/4a3642e971f404d69d4f6fa3885559d67562801b99d7592487f1ecc4e017/pip-20.3.3-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.5MB/s \n",
            "\u001b[?25hCollecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/78/56a7c88a57d0d14945472535d0df9fb4bbad7d34ede658ec7961635c790e/lxml-4.6.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22616 sha256=2528f8f29e60c3fdc173e9322f5b09c926768909850c67a37922df3546b46986\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vx2ksb1x/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: pip, lxml, yfinance\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.2 pip-20.3.3 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "source": [
        "num_iterations = 200000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 2  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-4  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f4T5fZKQpSI"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QrSckziQuQh"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1Hs4lwQG4_"
      },
      "source": [
        "## datas data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoJ9m8MujVle"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.stats import beta\n",
        "from sklearn import preprocessing\n",
        "from pandas_datareader import data as pdr\n",
        "#import yfinance as yf\n",
        "import fix_yahoo_finance as yf\n",
        "yf.pdr_override()\n",
        "import os \n",
        "from datetime import date"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5hfW9DmQOrE"
      },
      "source": [
        "start=\"2020-09-01\"\n",
        "end=\"2020-12-31\"\n",
        "#L=2500#periodo de los ultimos dias a analizar (maximo 250)\n",
        "M=0 #ultimo periodo que excluimos del registro para evitar el leakage\n",
        "la=32 #dias anteriores utilizados para calcular media y varianza del dia presente\n",
        "co=int(la/4)\n",
        "\n",
        "\n",
        "CO=\"CO\"\n",
        "\n",
        "LA=\"LA\" #string acorde con a"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJhhfjb-ZGXJ"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8KRzcLXLor3"
      },
      "source": [
        "class Betas():\n",
        "  def __init__(self,ticker,la=34,co=int(34/4),start=\"2010-01-01\",end=\"2020-12-31\"):\n",
        "    self.ticker=ticker\n",
        "    self.la=la\n",
        "    self.co=co\n",
        "    self.start=start\n",
        "    self.end=end\n",
        "    self.list=[\"Open\",\"High\",\"Low\",\"Close\"] \n",
        "    #self.list=[\"Close\"]\n",
        "  \n",
        "  def betas(self):\n",
        "    from pandas_datareader import data as pdr\n",
        "    from scipy.stats import beta\n",
        "    datau=pdr.get_data_yahoo(self.ticker,self.start,self.end,progress=False)\n",
        "    datau['LogReturn'] = np.log(datau['Close']).diff() \n",
        "    datau[\"P\"+\"LA\"]=0\n",
        "    datau[\"N\"+\"LA\"]=0\n",
        "    datau[\"P\"+\"CO\"]=0\n",
        "    datau[\"N\"+\"CO\"]=0\n",
        "\n",
        "    for i in self.list:\n",
        "      datau[i+\"R\"]=(datau[i]-datau[i].shift(1))*100/datau[i].shift(1)\n",
        "      datau[i+\"R\"]=datau[i+\"R\"].dropna()\n",
        "      datau[i+\"P\"] = datau[i+\"R\"].where(datau[i+\"R\"] > 0.0, 1e-6)\n",
        "      datau[i+\"N\"] = -datau[i+\"R\"].where(datau[i+\"R\"] <= 0.0, 1e-6)\n",
        "  \n",
        "      datau[i+\"P\"+\"LA\"]=datau[i+\"P\"].cumsum()-datau[i+\"P\"].shift(self.la).cumsum()+1.0\n",
        "      datau[i+\"N\"+\"LA\"]=datau[i+\"N\"].cumsum()-datau[i+\"N\"].shift(self.la).cumsum()+1.0\n",
        "      datau[i+\"P\"+\"CO\"]=datau[i+\"P\"].cumsum()-datau[i+\"P\"].shift(self.co).cumsum()+0.25\n",
        "      datau[i+\"N\"+\"CO\"]=datau[i+\"N\"].cumsum()-datau[i+\"N\"].shift(self.co).cumsum()+0.25\n",
        "\n",
        "      datau[\"P\"+\"LA\"]=datau[\"P\"+\"LA\"]+datau[i+\"P\"+\"LA\"]\n",
        "      datau[\"N\"+\"LA\"]=datau[\"N\"+\"LA\"]+datau[i+\"N\"+\"LA\"]\n",
        "      datau[\"P\"+\"CO\"]=datau[\"P\"+\"CO\"]+datau[i+\"P\"+\"CO\"]\n",
        "      datau[\"N\"+\"CO\"]=datau[\"N\"+\"CO\"]+datau[i+\"N\"+\"CO\"]\n",
        "\n",
        "    \n",
        "    datau=datau.dropna()    \n",
        "\n",
        "    datau[\"mean\"+\"LA\"]= beta.stats(datau[\"P\"+\"LA\"], datau[\"N\"+\"LA\"], moments='m')\n",
        "    datau[\"mean\"+\"CO\"]= beta.stats(datau[\"P\"+\"CO\"], datau[\"N\"+\"CO\"], moments='m')\n",
        "\n",
        "    \n",
        "\n",
        "    datau = datau.reset_index()    \n",
        "\n",
        "    \n",
        "    datau[\"meanCO-8\"]=datau[\"meanCO\"].shift(8)\n",
        "    datau[\"meanCO-16\"]=datau[\"meanCO\"].shift(16)\n",
        "    datau[\"meanCO-24\"]=datau[\"meanCO\"].shift(24)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    datau=datau[[\"Date\",\"Open\",\"Close\",\"meanLA\",\"meanCO\",\"LogReturn\",\"meanCO-8\",\"meanCO-16\",\"meanCO-24\"]]\n",
        "    datau=datau.dropna()\n",
        "    return datau"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFaadelrVs59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f837a06a-1723-406f-d4bf-0747ddaaa79d"
      },
      "source": [
        "#CARTERA REVISADA\n",
        "\n",
        "igbm=[\"A3M.MC\",\"ABG.MC\",\"ACS.MC\",\"ACX.MC\",\"ADX.MC\",\"ADZ.MC\",\"AEDAS.MC\",\"AENA.MC\",\"AI.MC\",\"AIR.MC\",\"ALB.MC\",\"ALM.MC\",\"ALNT.MC\",\"AMP.MC\",\"AMS.MC\",\"ANA.MC\",\"APAM.MC\",\"APPS.MC\",\"ARM.MC\",\n",
        "      \"AZK.MC\",\"BAIN.MC\",\"BBVA.MC\",\"BDL.MC\",\"BIO.MC\",\"BKIA.MC\",\"BKT.MC\",\"BKY.MC\",\"BME.MC\",\"CABK.MC\",\"CAF.MC\",\"CASH.MC\",\"CBAV.MC\",\"CCEP.MC\",\"CDR.MC\",\"CEV.MC\",\"CIE.MC\",\"CLEO.MC\",\"CLNX.MC\",\n",
        "      \"CMC.MC\",\"COL.MC\",\"DESA.MC\",\"DIA.MC\",\"DOM.MC\",\"EAT.MC\",\"EBRO.MC\",\"ECR.MC\",\"EDR.MC\",\"EKT.MC\",\"ELE.MC\",\"ENC.MC\",\"ENG.MC\",\"ENO.MC\",\"EZE.MC\",\"FAE.MC\",\"FCC.MC\",\"FDR.MC\",\"FER.MC\",\"GALQ.MC\",\n",
        "      \"GCO.MC\",\"GEST.MC\",\"GRE.MC\",\"GRF.MC\",\"GSJ.MC\",\"HOME.MC\",\"IAG.MC\",\"IBE.MC\",\"IBG.MC\",\"IDR.MC\",\"ISUR.MC\",\"ITX.MC\",\"LBK.MC\",\"LGT.MC\",\"LOG.MC\",\"LRE.MC\",\"MAP.MC\",\"MAS.MC\",\"MCM.MC\",\"MDF.MC\",\n",
        "      \"MEL.MC\",\"MRL.MC\",\"MTB.MC\",\"MTS.MC\",\"MVC.MC\",\"NEA.MC\",\"NHH.MC\",\"NTGY.MC\",\"NTH.MC\",\"NXT.MC\",\"NYE.MC\",\"OHL.MC\",\"OLE.MC\",\"ORY.MC\",\"PHM.MC\",\"PRM.MC\",\n",
        "      \"PRS.MC\",\"PSG.MC\",\"PVA.MC\",\"QBT.MC\",\"R4.MC\",\"RDM.MC\",\"REE.MC\",\"REN.MC\",\"REP.MC\",\"RIO.MC\",\"RJF.MC\",\"RLIA.MC\",\"ROVI.MC\",\"SAB.MC\",\"SAN.MC\",\"SCYR.MC\",\"SGRE.MC\",\n",
        "      \"SLR.MC\",\"SPK.MC\",\"SPS.MC\",\"TEF.MC\",\"TL5.MC\",\"TLGO.MC\",\"TRE.MC\",\"TRG.MC\",\"TUB.MC\",\"UBS.MC\",\"UNI.MC\",\"VER.MC\",\"VID.MC\",\"VIS.MC\",\"VOC.MC\",\"ZOT.MC\"]\n",
        "cac=[\"AC.PA\",\"AI.PA\",\"AIR.PA\",\"MT.AS\",\"ATO.PA\",\"CS.PA\",\"BNP.PA\",\"CAP.PA\",\"CA.PA\",\"BN.PA\",\"DSY.PA\",\"ENGI.PA\",\"EL.PA\",\"RMS.PA\",\"KER.PA\",\"OR.PA\",\"LR.PA\",\"MC.PA\",\"ML.PA\",\"ORA.PA\",\"RI.PA\",\"UG.PA\",\"PUB.PA\",\"RNO.PA\",\"SAF.PA\",\"SGO.PA\",\"SAN.PA\",\"SU.PA\",\"GLE.PA\",\"SW.PA\",\"STM.PA\",\"HO.PA\",\"FP.PA\",\"VIE.PA\",\"DG.PA\",\"VIV.PA\",\"WLN.PA\"]\n",
        "dow=[\"MCD\",\"PG\",\"JPM\",\"AAPL\",\"JPM\",\"GOOG\",\"GS\",\"TRV\",\"AXP\",\"PFE\",\"BA\",\"JNJ\",\"UNH\",\"VZ\",\"MRK\",\"NKE\",\"CSCO\",\"IBM\",\"CAT\",\"DIS\",\"KO\",\"WBA\",\"MSFT\",\"MMM\",\"INTC\",\"WMT\",\"XOM\",\"CVX\",\"HD\",\"ALXN\",\"BIDU\"]\n",
        "ibex=[\"ANA.MC\",\"ACX.MC\",\"ACS.MC\",\"AENA.MC\",\"ALM.MC\",\"AMS.MC\",\"MTS.MC\" ,\"SAB.MC\",\"SAN.MC\",\"BKIA.MC\",\"BKT.MC\",\"BBVA.MC\",\"CABK.MC\",\"CLNX.MC\",\"CIE.MC\",\"ENG.MC\",\"ELE.MC\",\"FER.MC\",\"GRF.MC\",\"IAG.MC\",\"IBE.MC\",\"ITX.MC\",\"IDR.MC\",\"COL.MC\",\"MAP.MC\",\"MEL.MC\",\"MRL.MC\",\"NTGY.MC\",\"PHM.MC\",\"REE.MC\",\"REP.MC\",\"SGRE.MC\",\"SLR.MC\",\"TEF.MC\",\"VIS.MC\"]\n",
        "#ibex=[\"BBVA.MC\",\"TL5.MC\",\"TEF.MC\",\"MRL.MC\",\"NTGY.MC\",\"REE.MC\",\"REP.MC\",\"SAN.MC\",\"TEF.MC\",\"VIS.MC\",\"ANA.MC\",\"ACX.MC\",\"ACS.MC\",\"AENA.MC\",\"MTS.MC\",\"BKT.MC\",\"BBVA.MC\",\"CABK.MC\",\"CLNX.MC\",\"COL.MC\",\"ELE.MC\",\"FER.MC\",\"IAG.MC\",\"IBE.MC\",\"ITX.MC\",\"IDR.MC\",\"MAP.MC\",\"GRF.MC\",\"MEL.MC\",\"SGRE.MC\"]\n",
        "dax=[\"BEI.DE\",\"FRE.DE\",\"LHA.DE\",\"SIE.DE\",\"DAI.DE\",\"VNA.DE\",\"WDI.DE\",\"DPW.DE\",\"RWE.DE\",\"HEI.DE\",\"MRK.DE\",\"SAP.DE\",\"ALV.DE\",\"IFX.DE\",\"FME.DE\",\"DB1.DE\",\"MUV2.DE\",\"EOAN.DE\",\"CON.DE\",\"BAYN.DE\",\"DBK.DE\",\"VOW3.DE\",\"ADS.DE\",\"BMW.DE\",\"DTE.DE\",\"HEN3.DE\"]\n",
        "nasdaq=[\"ATVI\",\"ADBE\",\"AMD\",\"ALXN\",\"ALGN\",\"GOOG\",\"GOOGL\",\"AMZN\",\"AMGN\",\"ADI\",\"ANSS\",\"AAPL\",\"AMAT\",\"ASML\",\"ADSK\",\"ADP\",\"BIDU\",\"BIIB\",\"BMRN\",\"BKNG\",\"AVGO\",\"CDNS\",\"CDW\",\"CERN\",\"CHTR\",\"CHKP\",\"CTAS\",\"CSCO\",\"CTXS\",\"CTSH\",\"CMCSA\",\"CPRT\",\n",
        "        \"CSGP\",\"COST\",\"CSX\",\"DXCM\",\"DLTR\",\"EBAY\",\"EA\",\"EXC\",\"EXPE\",\"FB\",\"FAST\",\"FISV\",\"GILD\",\"IDXX\",\"ILMN\",\"INCY\",\"INTC\",\"INTU\",\"ISRG\",\"JD\",\"KLAC\",\"LRCX\",\"LBTYA\",\"LBTYK\",\"LULU\",\"MAR\",\"MXIM\",\"MELI\",\"MCHP\",\"MU\",\"MSFT\",\n",
        "        \"MDLZ\",\"MNST\",\"NTAP\",\"NTES\",\"NFLX\",\"NVDA\",\"NXPI\",\"ORLY\",\"PCAR\",\"PAYX\",\"PYPL\",\"PEP\",\"QCOM\",\"REGN\",\"ROST\",\"SGEN\",\"SIRI\",\"SWKS\",\"SPLK\",\"SBUX\",\"SNPS\",\"TMUS\",\"TTWO\",\"TSLA\",\"TXN\",\"KHC\",\"TCOM\",\"ULTA\",\"UAL\",\"VRSN\",\"VRSK\",\"VRTX\",\n",
        "        \"WBA\",\"WDC\",\"WDAY\",\"XEL\",\"XLNX\"]\n",
        "s_and_p_100=[\"AA\",\"AAPL\",\"ABT\",\"AEP\",\"ALL\",\"AMZN\",\"AXP\",\"BA\",\"BAC\",\"BAX\",\"BK\",\"BMY\",\"C\",\"CAT\",\"CL\",\"CMCSA\",\"COF\",\"COP\",\"COST\",\"CPB\",\"CSCO\",\"CVS\",\"CVX\",\"DIS\",\"DVN\",\"ETR\",\"EXC\",\"F\",\"FCX\",\"FDX\",\"GD\",\"GE\",\n",
        "             \"GILD\",\"GOOG\",\"GS\",\"HAL\",\"HD\",\"HON\",\"HPQ\",\"IBM\",\"JNJ\",\"JPM\",\"KO\",\"LMT\",\"LOW\",\"MCD\",\"MDT\",\"MET\",\"MMM\",\"MO\",\"MON\",\"MRK\",\"MS\",\"MSFT\",\"NKE\",\"NOV\",\"NSC\",\"NWSA\",\"ORCL\",\"OXY\",\"PEP\",\"PFE\",\"PG\",\"PM\",\"QCOM\",\"RF\",\n",
        "             \"SLB\",\"SO\",\"T\",\"TGT\",\"TWX\",\"TXN\",\"UNH\",\"UPS\",\"USB\",\"VZ\",\"WFC\",\"WMB\",\"WMT\",\"WY\",\"XOM\",\"XRX\"]\n",
        "\n",
        "mib=[\"SPM.MI\",\"DIA.MI\",\"FBK.MI\",\"FCA.MI\",\"BAMI.MI\",\"PRY.MI\",\"AZM.MI\",\"TIT.MI\",\"UNI.MI\",\"BMED.MI\",\"BZU.MI\",\"RACE.MI\",\"LDO.MI\",\"MB.MI\",\"PIRC.MI\",\"ATL.MI\",\"ENI.MI\",\"G.MI\",\"PST.MI\",\"CPR.MI\",\"UBI.MI\",\"TEN.MI\",\"REC.MI\",\"IG.MI\",\"TRN.MI\",\"SRG.MI\",\"IP.MI\",\"CNHI.MI\",\"MONC.MI\",\"STM.MI\"]\n",
        "\n",
        "total=ibex+cac+dax+mib+dow+nasdaq+s_and_p_100\n",
        "europa=ibex+cac+dax+mib\n",
        "america=dow+nasdaq+s_and_p_100\n",
        "america2=nasdaq+s_and_p_100\n",
        "total2=ibex+cac+dax+mib+s_and_p_100+nasdaq\n",
        "mercados=[igbm,ibex,cac,dax,mib,dow,nasdaq,s_and_p_100]\n",
        "\n",
        "len(total)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "341"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKLEhCuxSAeW"
      },
      "source": [
        "mert=pd.DataFrame()\n",
        "\n",
        "for ticker in ibex:\n",
        "  mer=Betas(ticker,start=\"2020-10-10\",end=\"2021-12-31\").betas()\n",
        "  mer[\"ticker\"]=ticker\n",
        "  mert = pd.concat([mert, mer], ignore_index=True, sort=False)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1n8-GrXSpQ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e434419a-49b4-40ab-8fcb-23569451b508"
      },
      "source": [
        "data=mert.copy()\n",
        "L=len(data)\n",
        "\n",
        "data.head(12)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>meanLA</th>\n",
              "      <th>meanCO</th>\n",
              "      <th>LogReturn</th>\n",
              "      <th>meanCO-8</th>\n",
              "      <th>meanCO-16</th>\n",
              "      <th>meanCO-24</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-04</td>\n",
              "      <td>116.900002</td>\n",
              "      <td>117.099998</td>\n",
              "      <td>0.663225</td>\n",
              "      <td>0.789782</td>\n",
              "      <td>0.003422</td>\n",
              "      <td>0.475895</td>\n",
              "      <td>0.755104</td>\n",
              "      <td>0.746101</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-01-05</td>\n",
              "      <td>117.300003</td>\n",
              "      <td>116.500000</td>\n",
              "      <td>0.652152</td>\n",
              "      <td>0.677627</td>\n",
              "      <td>-0.005137</td>\n",
              "      <td>0.526091</td>\n",
              "      <td>0.745408</td>\n",
              "      <td>0.800470</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-01-06</td>\n",
              "      <td>117.300003</td>\n",
              "      <td>119.500000</td>\n",
              "      <td>0.669318</td>\n",
              "      <td>0.631131</td>\n",
              "      <td>0.025425</td>\n",
              "      <td>0.693920</td>\n",
              "      <td>0.583734</td>\n",
              "      <td>0.733978</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-01-07</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>126.099998</td>\n",
              "      <td>0.693347</td>\n",
              "      <td>0.787707</td>\n",
              "      <td>0.053759</td>\n",
              "      <td>0.668118</td>\n",
              "      <td>0.694615</td>\n",
              "      <td>0.588785</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-01-08</td>\n",
              "      <td>127.699997</td>\n",
              "      <td>127.900002</td>\n",
              "      <td>0.707187</td>\n",
              "      <td>0.837368</td>\n",
              "      <td>0.014173</td>\n",
              "      <td>0.689448</td>\n",
              "      <td>0.672709</td>\n",
              "      <td>0.632743</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021-01-11</td>\n",
              "      <td>127.099998</td>\n",
              "      <td>124.599998</td>\n",
              "      <td>0.682657</td>\n",
              "      <td>0.745728</td>\n",
              "      <td>-0.026140</td>\n",
              "      <td>0.658039</td>\n",
              "      <td>0.694207</td>\n",
              "      <td>0.707450</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2021-01-12</td>\n",
              "      <td>125.699997</td>\n",
              "      <td>125.300003</td>\n",
              "      <td>0.684349</td>\n",
              "      <td>0.769990</td>\n",
              "      <td>0.005602</td>\n",
              "      <td>0.567071</td>\n",
              "      <td>0.724846</td>\n",
              "      <td>0.682663</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-01-13</td>\n",
              "      <td>122.900002</td>\n",
              "      <td>126.699997</td>\n",
              "      <td>0.681642</td>\n",
              "      <td>0.736151</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.562351</td>\n",
              "      <td>0.670849</td>\n",
              "      <td>0.692718</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2021-01-14</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>126.900002</td>\n",
              "      <td>0.679634</td>\n",
              "      <td>0.766955</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.789782</td>\n",
              "      <td>0.475895</td>\n",
              "      <td>0.755104</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2021-01-15</td>\n",
              "      <td>126.599998</td>\n",
              "      <td>125.400002</td>\n",
              "      <td>0.663248</td>\n",
              "      <td>0.739913</td>\n",
              "      <td>-0.011891</td>\n",
              "      <td>0.677627</td>\n",
              "      <td>0.526091</td>\n",
              "      <td>0.745408</td>\n",
              "      <td>ANA.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2021-01-04</td>\n",
              "      <td>9.180000</td>\n",
              "      <td>9.294000</td>\n",
              "      <td>0.640287</td>\n",
              "      <td>0.799868</td>\n",
              "      <td>0.028374</td>\n",
              "      <td>0.496460</td>\n",
              "      <td>0.386899</td>\n",
              "      <td>0.908947</td>\n",
              "      <td>ACX.MC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021-01-05</td>\n",
              "      <td>9.310000</td>\n",
              "      <td>9.312000</td>\n",
              "      <td>0.612701</td>\n",
              "      <td>0.727572</td>\n",
              "      <td>0.001935</td>\n",
              "      <td>0.616520</td>\n",
              "      <td>0.527629</td>\n",
              "      <td>0.601171</td>\n",
              "      <td>ACX.MC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date        Open       Close  ...  meanCO-16  meanCO-24  ticker\n",
              "0  2021-01-04  116.900002  117.099998  ...   0.755104   0.746101  ANA.MC\n",
              "1  2021-01-05  117.300003  116.500000  ...   0.745408   0.800470  ANA.MC\n",
              "2  2021-01-06  117.300003  119.500000  ...   0.583734   0.733978  ANA.MC\n",
              "3  2021-01-07  120.000000  126.099998  ...   0.694615   0.588785  ANA.MC\n",
              "4  2021-01-08  127.699997  127.900002  ...   0.672709   0.632743  ANA.MC\n",
              "5  2021-01-11  127.099998  124.599998  ...   0.694207   0.707450  ANA.MC\n",
              "6  2021-01-12  125.699997  125.300003  ...   0.724846   0.682663  ANA.MC\n",
              "7  2021-01-13  122.900002  126.699997  ...   0.670849   0.692718  ANA.MC\n",
              "8  2021-01-14  127.000000  126.900002  ...   0.475895   0.755104  ANA.MC\n",
              "9  2021-01-15  126.599998  125.400002  ...   0.526091   0.745408  ANA.MC\n",
              "10 2021-01-04    9.180000    9.294000  ...   0.386899   0.908947  ACX.MC\n",
              "11 2021-01-05    9.310000    9.312000  ...   0.527629   0.601171  ACX.MC\n",
              "\n",
              "[12 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CROu0IDrDKHJ"
      },
      "source": [
        "#plt.scatter(data[\"Tendencia\"], data[\"LogReturn\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZySGXYFSTL1d"
      },
      "source": [
        "## Betas environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhtZ6dSzTiFk"
      },
      "source": [
        "\n",
        "\n",
        "import abc\n",
        "import six\n",
        "\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlD2Dd2vUTtg"
      },
      "source": [
        "class PyEnvironment(object):\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "    self._current_time_step = self._reset()\n",
        "    return self._current_time_step\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\"\n",
        "    if self._current_time_step is None:\n",
        "        return self.reset()\n",
        "    self._current_time_step = self._step(action)\n",
        "    return self._current_time_step\n",
        "\n",
        "  def current_time_step(self):\n",
        "    return self._current_time_step\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Return time_step_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Return observation_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def action_spec(self):\n",
        "    \"\"\"Return action_spec.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Return initial_time_step.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Apply action and return new time_step.\"\"\"\n",
        "    self._current_time_step = self._step(action)\n",
        "    return self._current_time_step"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiZ826WNWIET"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm5avwM0jcNl"
      },
      "source": [
        "# ATENCION: Todas las variables deben ir precedidas con self._\n",
        "\n",
        "class BetamEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self,data):\n",
        "    self._C=5000\n",
        "    #self._ticker_comprado=self._data.loc[0,\"ticker\"]  #atributo para evitar que se realicen operaciones de venta con un accion diferente a la que se haya comprado\n",
        "    self._IA=0       #inversion acumulada\n",
        "    self._VA=0       #valor actual de la accion\n",
        "    self._VC=0        #valor de compra de la accion\n",
        "    self._na=0        # numero acciones compradas\n",
        "    self._i=3\n",
        "    self._reward=0\n",
        "    self._data=data\n",
        "    self._invested=0\n",
        "    self._Lon=len(self._data)-2\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,7), dtype=np.float32, minimum=0, maximum=1,  name='observation')\n",
        "    self._state=self._data.loc[0,[\"meanLA\",\"meanCO\",\"meanLA-4\",\"meanLA-8\",\"meanLA-16\", \"meanCO-2\",\"meanCO-4\"]].values\n",
        "\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state=self._data.loc[0,[\"meanLA\",\"meanCO\",\"meanLA-4\",\"meanLA-8\",\"meanLA-16\", \"meanCO-2\",\"meanCO-4\"]].values\n",
        "    self._i=3\n",
        "    self._IA=0\n",
        "    self._VA=0\n",
        "    self._VC=0\n",
        "    self._na=0\n",
        "    self._invested=0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array([self._state], dtype=np.float32))\n",
        "   \n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._i>=self._Lon:\n",
        "        self._episode_ended = True\n",
        "\n",
        "\n",
        "\n",
        "#ACCION 2\n",
        "    if action == 2  and self._IA<=3*self._C:\n",
        "      self._VC=self._VC+self._data.loc[self._i+1,\"Open\"]*int(self._C/self._data.loc[self._i+1,\"Open\"])\n",
        "      self._na=self._na+int(self._C/self._data.loc[self._i+1,\"Open\"])\n",
        "      self._VA=self._VC      \n",
        "      self._IA+=self._VC\n",
        "      #self._reward=0.0\n",
        "      self._invested=1\n",
        "\n",
        "    elif action == 2 :         \n",
        "      self._na=int(self._C/self._data.loc[self._i+1,\"Open\"])\n",
        "      self._VA=self._data.loc[self._i+1,\"Open\"]*self._na\n",
        "      self._VC=self._data.loc[self._i+1,\"Open\"]*self._na\n",
        "      self._IA+=self._VC\n",
        "      #self._reward=0.0   \n",
        "      self._invested=1\n",
        "# ACCION 1\n",
        "    #elif action == 1  and self._VC>0:\n",
        "      #self._reward=self._data.loc[self._i+1,\"Open\"]*self._na-self._VC\n",
        "      #self._IA-=self._VC\n",
        "      #self._IA=self._IA\n",
        "      #self._VC=0\n",
        "      #self._na=0\n",
        "      #self._VA=0\n",
        "      #self._invested=0\n",
        "    elif action == 1:\n",
        "      self._IA=self._IA\n",
        "      self._VC=self._VC \n",
        "      self._na=self._na\n",
        "      self._VA=self._VA \n",
        "      #self._reward=0        \n",
        "      self._invested=0\n",
        " \n",
        " # ACCION 0\n",
        "    elif action==0:          \n",
        "      self._VC=self._VC\n",
        "      self._IA=self._IA\n",
        "      self._na=self._na\n",
        "      self._VA=self._na*self._data.loc[self._i,\"Close\"] \n",
        "      #self._reward=0 \n",
        "        \n",
        "    else:\n",
        "          \n",
        "      raise ValueError('`action` should be 0, 1, or 2')\n",
        "    # compute reward\n",
        "    if self._invested:\n",
        "      self._reward=(self._data.loc[self._i,\"LogReturn\"])*self._na\n",
        "    else:\n",
        "      self._reward = 0\n",
        "    # baseline\n",
        "    #self._total_buy_and_hold += self._df.loc[self._current_idx,['LogReturn']].values\n",
        "    if self._episode_ended:\n",
        "      self._IA=0\n",
        "      self._VC=0\n",
        "      self._na=0\n",
        "      self._i=0\n",
        "      self._reward = self._reward\n",
        "      self._invested=0\n",
        "      self._state=self._data.loc[0,[\"meanLA\",\"meanCO\",\"meanLA-4\",\"meanLA-8\",\"meanLA-16\", \"meanCO-2\",\"meanCO-4\"]].values\n",
        "      return ts.termination(np.array([self._state], dtype=np.float32), self._reward)\n",
        "    \n",
        "    else:\n",
        "      self._i=self._i+1\n",
        "      self._IA=self._IA\n",
        "      self._VC=self._VC\n",
        "      self._na=self._na\n",
        "      self._VA=self._na*self._data.loc[self._i,\"Close\"]\n",
        "      self._reward=self._reward\n",
        "      self._invested=self._invested\n",
        "      self._state = self._data.loc[self._i,[\"meanLA\",\"meanCO\",\"meanLA-4\",\"meanLA-8\",\"meanLA-16\", \"meanCO-2\",\"meanCO-4\"]].values\n",
        "\n",
        "\n",
        "\n",
        "      return ts.transition(np.array([self._state], dtype=np.float32), self._reward)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVzdT0XeOXT4"
      },
      "source": [
        "class EnEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self, df):\n",
        "    self._df = df\n",
        "    self._n = len(df)-1\n",
        "    self._current_idx = 0\n",
        "    self._total_buy_and_hold = 0\n",
        "    self._invested = 0\n",
        "    \n",
        "    #self._states = self._df[feats].to_numpy()\n",
        "    self._reward = self._df['LogReturn']#.to_numpy()\n",
        "    \n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,5), dtype=np.float32, minimum=0, maximum=1,  name='observation')\n",
        "    self._state=self._df.loc[0,[\"meanLA\",\"meanCO\",\"meanCO-8\",\"meanCO-16\",\"meanCO-24\"]].values\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec \n",
        "\n",
        "  def _reset(self):\n",
        "    self._current_idx = 0\n",
        "    self._total_buy_and_hold = 0\n",
        "    self._invested = 0\n",
        "    self._reward=0\n",
        "    self._state=self._df.loc[0,[\"meanLA\",\"meanCO\",\"meanCO-8\",\"meanCO-16\",\"meanCO-24\"]].values\n",
        "    return ts.restart(np.array([self._state], dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    # need to return (next_state, reward, done)\n",
        "    \n",
        "    if self._current_idx >=self._n:\n",
        "      self._current_idx = 0\n",
        "      self._total_buy_and_hold = 0\n",
        "      self._invested = 0\n",
        "      self._reward=0\n",
        "      self._state=self._df.loc[0,[\"meanLA\",\"meanCO\",\"meanCO-8\",\"meanCO-16\",\"meanCO-24\"]].values\n",
        "      return ts.termination(np.array([self._state], dtype=np.float32), self._reward)\n",
        "    \n",
        "    if self._current_idx >= self._n:\n",
        "      raise Exception(\"Episode already done\")\n",
        "\n",
        "    if action == 0: # BUY\n",
        "      self._invested = 1\n",
        "    elif action == 1: # SELL\n",
        "      self._invested = 0\n",
        "    elif action == 2: # DO NOTHING\n",
        "      self._invested = self._invested    \n",
        "    # compute reward\n",
        "    if self._invested:\n",
        "      self._reward = self._df.loc[self._current_idx,'LogReturn']\n",
        "    else:\n",
        "      self._reward = 0\n",
        "    # baseline\n",
        "    self._total_buy_and_hold += self._df.loc[self._current_idx,['LogReturn']].values\n",
        "\n",
        "    # state transition\n",
        "    \n",
        "    self._state = self._df.loc[self._current_idx,[\"meanLA\",\"meanCO\",\"meanCO-8\",\"meanCO-16\",\"meanCO-24\"]].values\n",
        "    self._current_idx = self._current_idx+1\n",
        "    return ts.transition(np.array([self._state], dtype=np.float32), self._reward)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2uO_-Fb325d"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
        "\n",
        "Load the CartPole environment from the OpenAI Gym suite. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEz-S9gEv2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc19b179-371a-4739-d8e0-ed69b3fb2f93"
      },
      "source": [
        "env = EnEnv(data)\n",
        "print('action_spec:', env.action_spec())\n",
        "print(\"\")\n",
        "\n",
        "print(env.time_step_spec())\n",
        "print(\"\")\n",
        "print('time_step_spec.observation:', env.time_step_spec().observation)\n",
        "print('time_step_spec.step_type:', env.time_step_spec().step_type)\n",
        "print('time_step_spec.discount:', env.time_step_spec().discount)\n",
        "print('time_step_spec.reward:', env.time_step_spec().reward)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=2)\n",
            "\n",
            "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(1, 5), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0))\n",
            "\n",
            "time_step_spec.observation: BoundedArraySpec(shape=(1, 5), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n",
            "time_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n",
            "time_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
            "time_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
        "\n",
        "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bfff82-0031-432a-d832-3bfbfadce542"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Spec:\n",
            "BoundedArraySpec(shape=(1, 5), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxiSyCbBUQPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ac4fc7-2af5-4d1d-dc6b-1cfe40f6a2b3"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward Spec:\n",
            "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bttJ4uxZUQBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4540f0c-bc63-45e2-e8ad-0d5c1bbb487e"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Spec:\n",
            "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2UGR5t_iZX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b73901fb-9d55-4b8b-c5a2-1c7604e55c5e"
      },
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time step:\n",
            "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[0.663225  , 0.7897817 , 0.4758948 , 0.75510406, 0.7461012 ]],\n",
            "      dtype=float32))\n",
            "Next time step:\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[0.663225  , 0.7897817 , 0.4758948 , 0.75510406, 0.7461012 ]],\n",
            "      dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Usually two environments are instantiated: one for training and one for evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7brXNIGWXjC"
      },
      "source": [
        "train_py_env = EnEnv(data)\n",
        "eval_py_env = EnEnv(data)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
        "\n",
        "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AdtSqxZn0Db"
      },
      "source": [
        "tf.random.set_seed(200)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "\n",
        "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
        "\n",
        "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
        "-   [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
        "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
        "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
        "\n",
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "\n",
        "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "Use `tf_agents.networks.q_network` to create a `QNetwork`, passing in the `observation_spec`, `action_spec`, and a tuple describing the number and size of the model's hidden layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "source": [
        "fc_layer_params = (400,300)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "tf_agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "tf_agent.initialize()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n",
        "\n",
        "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
        "\n",
        "In this tutorial:\n",
        "\n",
        "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
        "-   The policy returns an action (left or right) for each `time_step` observation.\n",
        "\n",
        "Agents contain two policies: \n",
        "\n",
        "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
        "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "source": [
        "eval_policy = tf_agent.policy\n",
        "collect_policy = tf_agent.collect_policy"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qs1Fl3dV0ae"
      },
      "source": [
        "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE37-UCIrE69"
      },
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlnlRRsUbxP"
      },
      "source": [
        "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
        "\n",
        "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
        "-   `state` — used for stateful (that is, RNN-based) policies\n",
        "-   `info` — auxiliary data, such as log probabilities of actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DHZtq3Ndis"
      },
      "source": [
        "time_step = env.reset()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRFqAUzpNaAW"
      },
      "source": [
        "#random_policy.action(time_step)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bgU6Q6BZ8Bp"
      },
      "source": [
        "#compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
        "\n",
        "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=tf_agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZ-3HcqgE1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73f558c-51ee-4432-bb8f-f79eafeab0dc"
      },
      "source": [
        "tf_agent.collect_data_spec"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(1, 5), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy6g1tGcfRlw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942d2c3d-b407-49c4-851d-4a147185e410"
      },
      "source": [
        "tf_agent.collect_data_spec._fields"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr1KSAEGG4h9"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations. \n",
        "# For more details see the drivers module.\n",
        "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZnLu2ViO4E"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujU-PMUsKjS"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7bilizt_qW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3f72d5-aa92-432f-b2e7-4e1202466482"
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "\n",
        "dataset"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 1, 5), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.float32, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13AST-2ppOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f0b4eb-d8d4-474e-f854-74b90360973a"
      },
      "source": [
        "iterator = iter(dataset)\n",
        "\n",
        "print(iterator)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fcd7ff62cc0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th5w5Sff0b16"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data \n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3hBLhTpRFRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee906e32-f49d-485b-8f07-fa2f5b8341ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqXTuX4QVEFX"
      },
      "source": [
        "from tf_agents.policies import policy_saver\r\n",
        "from tf_agents.policies import py_tf_eager_policy\r\n",
        "from tf_agents.policies import random_tf_policy"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1So8AKejRAdg"
      },
      "source": [
        "policy_dir_best = os.path.join(\"/content/drive/MyDrive/policy_ibex\", 'policy')\r\n",
        "\r\n",
        "tf_policy_saver = policy_saver.PolicySaver(tf_agent.policy)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "-   collect data from the environment\n",
        "-   use that data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodicially evaluates the policy and prints the current score.\n",
        "\n",
        "The following will take ~5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq0FooUP0tjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4926f5-20a5-4a99-ffcd-9810313e4a40"
      },
      "source": [
        "\r\n",
        "try:\r\n",
        "  %%time\r\n",
        "except:\r\n",
        "  pass\r\n",
        "\r\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\r\n",
        "tf_agent.train = common.function(tf_agent.train)\r\n",
        "\r\n",
        "# Reset the train step\r\n",
        "tf_agent.train_step_counter.assign(0)\r\n",
        "\r\n",
        "# Evaluate the agent's policy once before training.\r\n",
        "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\r\n",
        "returns = [avg_return]\r\n",
        "avg_return_b=0\r\n",
        "for _ in range(num_iterations):\r\n",
        "\r\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\r\n",
        "  collect_data(train_env, tf_agent.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n",
        "\r\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\r\n",
        "  experience, unused_info = next(iterator)\r\n",
        "  train_loss = tf_agent.train(experience).loss\r\n",
        "\r\n",
        "  step = tf_agent.train_step_counter.numpy()\r\n",
        "\r\n",
        "  #if step % log_interval == 0:\r\n",
        "    #print('step = {0}: loss = {1}'.format(step, train_loss))\r\n",
        "\r\n",
        "  if step % eval_interval == 0:\r\n",
        "    avg_return_i = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\r\n",
        "    if avg_return_i>=avg_return_b:\r\n",
        "      avg_return_b=avg_return_i\r\n",
        "      tf_policy_saver.save(policy_dir_best)\r\n",
        "      print('step = {0}: Average Return Best = {1}'.format(step, avg_return_b))\r\n",
        "      returns.append(avg_return_b)\r\n",
        "    else:\r\n",
        "      print('step = {0}: Average Return = {1}'.format(step, avg_return_i))\r\n",
        "      returns.append(avg_return_i)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.72 µs\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/policy_ibex/policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/policy_ibex/policy/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step = 1000: Average Return Best = 0.7617636322975159\n",
            "step = 2000: Average Return = 0.2876186966896057\n",
            "step = 3000: Average Return = 0.6011011004447937\n",
            "step = 4000: Average Return = 0.6428426504135132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
        "\n",
        "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxtL1mbOYCVO"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh-QGpJ_i42O"
      },
      "source": [
        "## Aplicamos la policy a los diferentes tickers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHfHnfBFSlzf"
      },
      "source": [
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "474ecEiaT3G0"
      },
      "source": [
        "best_policy = tf.compat.v2.saved_model.load(policy_dir_best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZBVGO2NjaBH"
      },
      "source": [
        "\n",
        "def run_one_episodes(policy, eval_env,ticker):\n",
        "  action_s=[]\n",
        "\n",
        "  time_step = eval_env_accion.reset()\n",
        "  ret=0\n",
        "  while not time_step.is_last():\n",
        "    action_step = policy.action(time_step)\n",
        "    time_step = eval_env_accion.step(action_step.action)\n",
        "\n",
        "    ret+=time_step.reward\n",
        "    rent=ret.numpy()\n",
        "    accion= action_step[0].numpy()\n",
        "    action_s.append(accion[0])\n",
        "\n",
        "\n",
        "\n",
        "  print(ticker,rent,len(action_s),action_s,\"Recomendacion: \", accion)\n",
        "  return rent[0],accion,ticker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKu4GWNQL_zK"
      },
      "source": [
        "start=\"2020-10-10\"\n",
        "end=\"2021-5-30\"\n",
        "Return=0\n",
        "compras=[]\n",
        "ventas=[]\n",
        "mantener=[]\n",
        "for ticker in ibex:\n",
        "  #action_s=[]\n",
        "\n",
        "  data_ticker=Betas(ticker,start=start,end=end).betas()\n",
        "  data_ticker.reset_index(inplace=True)\n",
        "  data_ticker[\"ticker\"]=ticker\n",
        "  env_accion=EnEnv(data_ticker) #este es el objeto entorno con las propiedades y metodos (entre ellos el metodo step)\n",
        "  #utils.validate_py_environment(env_accion, episodes=5)\n",
        "  eval_env_accion = tf_py_environment.TFPyEnvironment(env_accion)\n",
        "\n",
        "  #aplicamos la mejor policia  \n",
        "  rentabilidad,accion,ticker=run_one_episodes(best_policy, eval_env_accion,ticker)\n",
        "  Return=Return+rentabilidad\n",
        "  if accion==0:\n",
        "    compras.append(ticker)\n",
        "  elif accion==1:\n",
        "    ventas.append(ticker)\n",
        "  elif accion==2:\n",
        "    mantener.append(ticker)\n",
        "\n",
        "  eval_env_accion.reset()\n",
        "print(\"\")\n",
        "print(\"Rentablidad del periodo y mercado\",Return)\n",
        "print(\"\")\n",
        "print(\"Valores a comprar: \",compras)\n",
        "print(\"Valores a vender: \",ventas)\n",
        "print(\"Valores a mantener: \",mantener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqMwYmw9YbQE"
      },
      "source": [
        "start=\"2020-10-16\"\r\n",
        "end=\"2021-5-30\"\r\n",
        "Return=0\r\n",
        "compras=[]\r\n",
        "ventas=[]\r\n",
        "mantener=[]\r\n",
        "for ticker in ibex:\r\n",
        "  #action_s=[]\r\n",
        "\r\n",
        "  data_ticker=Betas(ticker,start=start,end=end).betas()\r\n",
        "  data_ticker.reset_index(inplace=True)\r\n",
        "  data_ticker[\"ticker\"]=ticker\r\n",
        "  env_accion=EnEnv(data_ticker) #este es el objeto entorno con las propiedades y metodos (entre ellos el metodo step)\r\n",
        "  #utils.validate_py_environment(env_accion, episodes=5)\r\n",
        "  eval_env_accion = tf_py_environment.TFPyEnvironment(env_accion)\r\n",
        "\r\n",
        "  #aplicamos la mejor policia  \r\n",
        "  rentabilidad,accion,ticker=run_one_episodes(tf_agent.policy, eval_env_accion,ticker)\r\n",
        "  Return=Return+rentabilidad\r\n",
        "  if accion==0:\r\n",
        "    compras.append(ticker)\r\n",
        "  elif accion==1:\r\n",
        "    ventas.append(ticker)\r\n",
        "  elif accion==2:\r\n",
        "    mantener.append(ticker)\r\n",
        "\r\n",
        "  eval_env_accion.reset()\r\n",
        "print(\"\")\r\n",
        "print(\"Rentablidad del periodo y mercado\",Return)\r\n",
        "print(\"\")\r\n",
        "print(\"Valores a comprar: \",compras)\r\n",
        "print(\"Valores a vender: \",ventas)\r\n",
        "print(\"Valores a mantener: \",mantener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arqBBkEjts7C"
      },
      "source": [
        "start=\"2020-10-16\"\n",
        "end=\"2021-5-30\"\n",
        "Return=0\n",
        "for ticker in ibex:\n",
        "  action_s=[]\n",
        "  data_ticker=Betas(ticker,start=start,end=end).betas()\n",
        "  data_ticker.reset_index(inplace=True)\n",
        "  data_ticker[\"ticker\"]=ticker\n",
        "  rentabilidad=data_ticker[\"LogReturn\"].sum()\n",
        "  Return=Return+rentabilidad\n",
        "  eval_env_accion.reset()\n",
        "print(\"\")\n",
        "print(\"Rentabilidad buy and hold\",Return)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHE4SwqyNPS4"
      },
      "source": [
        "policy_dir = os.path.join(\"tutorials/tf2_course/Archivo/policy\", 'policy')\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(tf_agent.policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MDyNeYBY6NC"
      },
      "source": [
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJqW_xJ-ZF9q"
      },
      "source": [
        "saved_policy = tf.compat.v2.saved_model.load(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v40RxxZCbLa1"
      },
      "source": [
        "saved_policy.action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu0eKbXkxQQr"
      },
      "source": [
        "time_step = eval_env_accion.reset()\n",
        "action_step = saved_policy.action(time_step)\n",
        "time_step = eval_env_accion.step(action_step.action)\n",
        "accion=action_step[0][0].numpy()\n",
        "reward=time_step[1][0].numpy()\n",
        "accion, reward, time_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuHDdM91T4wZ"
      },
      "source": [
        "eval_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yrDAxFsfx4U"
      },
      "source": [
        "def run_one_episod(policy, eval_env,ticker, data_ticker,L):\n",
        "  #eval_env_accion = tf_py_environment.TFPyEnvironment(env_accion)\n",
        "  eval_env_accion = tf_py_environment.TFPyEnvironment(eval_env)\n",
        "  time_step = eval_env_accion.reset()\n",
        "  obs=eval_env_accion.reset()[3][0][0].numpy()\n",
        "  ret=eval_env_accion.reset()[1][0].numpy()\n",
        "  \n",
        "  operaciones=pd.DataFrame() \n",
        "  contador=1\n",
        "  data_ticker.loc[contador-1,\"invertido\"]=False\n",
        "  I=5000\n",
        "\n",
        "\n",
        "  while contador <=L-1: \n",
        "  #while not time_step.is_last():\n",
        "    \n",
        "    #action= policy(obs)\n",
        "    action_step = policy.action(time_step)\n",
        "    #time_step = eval_env_accion.step(action_step.action)\n",
        "    time_step = eval_env_accion.step(action_step.action)   \n",
        "\n",
        "    time_step_type = time_step[0][0].numpy()\n",
        "    obs=time_step[3][0][0].numpy()\n",
        "    ret=time_step[1][0].numpy()\n",
        "    accion=action_step[0][0].numpy()\n",
        "    LogReturn=data_ticker.loc[contador,\"LogReturn\"]\n",
        "    \n",
        "    if accion==0:\n",
        "      if data_ticker.loc[contador-1,\"invertido\"]==False:\n",
        "        ret=float(ret)\n",
        "        data_ticker.loc[contador,\"accion\"]=accion\n",
        "        data_ticker.loc[contador,\"invertido\"]=True           \n",
        "        data_ticker.loc[contador,\"inversion\"]=I\n",
        "        data_ticker.loc[contador,\"ganancia\"]=0\n",
        "\n",
        "      else:\n",
        "        ret=float(ret)\n",
        "        data_ticker.loc[contador,\"accion\"]=accion\n",
        "        data_ticker.loc[contador,\"invertido\"]=True      \n",
        "        data_ticker.loc[contador,\"inversion\"]=0\n",
        "        data_ticker.loc[contador,\"ganancia\"]=LogReturn\n",
        "    if accion==1:\n",
        "      if data_ticker.loc[contador-1,\"invertido\"]==True:\n",
        "        ret=float(ret)\n",
        "        data_ticker.loc[contador,\"accion\"]=accion\n",
        "        data_ticker.loc[contador,\"invertido\"]=False      \n",
        "        data_ticker.loc[contador,\"inversion\"]=-I\n",
        "        data_ticker.loc[contador,\"ganancia\"]=LogReturn\n",
        "      else:\n",
        "        ret=float(ret)\n",
        "        data_ticker.loc[contador,\"accion\"]=accion\n",
        "        data_ticker.loc[contador,\"invertido\"]=False      \n",
        "        data_ticker.loc[contador,\"inversion\"]=0\n",
        "        data_ticker.loc[contador,\"ganancia\"]=0     \n",
        "    elif accion==2:\n",
        "      if  data_ticker.loc[contador,\"invertido\"]==True:\n",
        "        ret=float(ret)\n",
        "        data_ticker.loc[contador,\"accion\"]=accion\n",
        "        data_ticker.loc[contador,\"invertido\"]=True      \n",
        "        data_ticker.loc[contador,\"inversion\"]=0\n",
        "        data_ticker.loc[contador,\"ganancia\"]=LogReturn\n",
        "      else:\n",
        "        ret=float(ret)\n",
        "        data_ticker.loc[contador,\"accion\"]=accion\n",
        "        data_ticker.loc[contador,\"invertido\"]=False      \n",
        "        data_ticker.loc[contador,\"inversion\"]=0\n",
        "        data_ticker.loc[contador,\"ganancia\"]=0\n",
        "\n",
        "    ticker=data_ticker.loc[contador,\"ticker\"]\n",
        "    fecha=data_ticker.loc[contador,\"Date\"]\n",
        "    cotizacion=data_ticker.loc[contador,\"Close\"]\n",
        "    accion=data_ticker.loc[contador,\"accion\"]\n",
        "    invertido=data_ticker.loc[contador,\"invertido\"]\n",
        "\n",
        "    ganancia=data_ticker.loc[contador,\"ganancia\"]\n",
        "    inversion=data_ticker.loc[contador,\"inversion\"]\n",
        "    meanLA=data_ticker.loc[contador,\"meanLA\"]\n",
        "    meanCO=data_ticker.loc[contador,\"meanCO\"]\n",
        "    #Posteriori2=data_ticker.loc[contador,\"Posteriori2\"]\n",
        "    #Posteriori3=data_ticker.loc[contador,\"Posteriori3\"]        \n",
        "    LogReturn=data_ticker.loc[contador,\"LogReturn\"]\n",
        "    \n",
        "\n",
        "    df=pd.DataFrame([[ticker,fecha, cotizacion, accion, invertido, ganancia, inversion, meanLA, meanCO,LogReturn]],columns=[\"ticker\",\"fecha\",\"cotizacion\",\"accion\",\"invertido\",\"ganancia\",\"inversion\",\"meanLA\",\"meanCO\",\"LogReturn\"])\n",
        "\n",
        "    operaciones=operaciones.append(df,ignore_index=True)\n",
        "\n",
        "  \n",
        "    contador+=1\n",
        "  return operaciones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kxAQkBpf4jX"
      },
      "source": [
        "start=\"2020-10-01\"\n",
        "end=\"2021-12-31\"\n",
        "\n",
        "Operaciones=pd.DataFrame()\n",
        "for ticker in ibex:\n",
        "\n",
        "  data_ticker1=Betas(ticker,start=start,end=end).betas()\n",
        "  data_ticker=data_ticker1.iloc[-250:].copy()\n",
        "  data_ticker.reset_index(inplace=True)\n",
        "  \n",
        "  data_ticker[\"ticker\"]=ticker\n",
        "  data_ticker['Date_'] =  pd.to_datetime(data_ticker.Date)\n",
        "  L=len(data_ticker)\n",
        "  env=EnEnv(data_ticker)\n",
        "\n",
        "  operaciones7=run_one_episod(best_policy, env,ticker,data_ticker,L)\n",
        "\n",
        "\n",
        "  Operaciones=Operaciones.append([operaciones7],ignore_index=True)\n",
        "\n",
        "Operaciones.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX9ubdeyZcT-"
      },
      "source": [
        "Operaciones.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1PSeoAAfBQo"
      },
      "source": [
        "Operaciones.sort_values(\"fecha\",inplace=True)\n",
        "Operaciones[Operaciones[\"invertido\"]==True]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp4j9S71dyoj"
      },
      "source": [
        "GananciaTotal=Operaciones[\"ganancia\"].sum()\n",
        "InversionFinal=Operaciones[\"inversion\"].max()\n",
        "print(GananciaTotal)\n",
        "print(InversionFinal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKOUEg3DJnJH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYsU_xe3p0Ut"
      },
      "source": [
        "\n",
        "Acciones=Operaciones[(Operaciones[\"accion\"]==0)].count()\n",
        "Acciones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZsL9XDlLXJU"
      },
      "source": [
        "Operaciones.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FP9o4WXGLuj"
      },
      "source": [
        "Operaciones.sort_values(\"fecha\",inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ8fZzb6z7gT"
      },
      "source": [
        "Operaciones['inversion_acum'] =Operaciones['inversion'].cumsum()\n",
        "Operaciones['ganancia_acum'] =Operaciones['ganancia'].cumsum()\n",
        "Operaciones[\"resultado\"]=np.where((Operaciones['ganancia']>=0) , 1, 0)\n",
        "Operaciones[\"resultado +\"]=np.where((Operaciones['ganancia']>0)  , 1, 0)\n",
        "Operaciones[\"resultado -\"]=np.where((Operaciones['ganancia']<0)  , 1, 0)\n",
        "Operaciones=Operaciones.copy()\n",
        "Operaciones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVhEl6wpFZDU"
      },
      "source": [
        "Operaciones[\"inversion_acum\"].max(),Operaciones.loc[Operaciones.index[-1],\"ganancia_acum\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FQ7JYf1JxZM"
      },
      "source": [
        "Periodo=Operaciones.loc[Operaciones.index[-1],\"fecha\"]-Operaciones.loc[Operaciones.index[0],\"fecha\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hClpu1O0liMM"
      },
      "source": [
        "Periodo=Operaciones.loc[Operaciones.index[-1],\"fecha\"]-Operaciones.loc[Operaciones.index[0],\"fecha\"]\n",
        "Inversion_maxima=Operaciones[\"inversion_acum\"].max()\n",
        "Ganancia_acumulada=Operaciones.loc[Operaciones.index[-1],\"ganancia_acum\"]\n",
        "Rentabilidad_periodo= Ganancia_acumulada*100/Inversion_maxima\n",
        "Ratio_operaciones_positivas=Operaciones[\"resultado +\"].sum()*100/(Operaciones[\"resultado +\"].sum()+Operaciones[\"resultado -\"].sum())\n",
        "Compras=len(Operaciones[(Operaciones[\"inversion\"]>0) & (Operaciones[\"accion\"]==0)])\n",
        "Beneficio_x_compra= Ganancia_acumulada*5000/Compras\n",
        "print(\"Periodo analizado:\",Periodo,Operaciones.loc[Operaciones.index[0],\"fecha\"],Operaciones.loc[Operaciones.index[-1],\"fecha\"] )\n",
        "print(\"Total compras realizadas             \",Compras)\n",
        "print(\"Inversion Maxima:                 \",\"{:.0f}\".format(Inversion_maxima ))\n",
        "print(\"Rentabilidad Periodo (%):          \",\"{:.3f}\".format(Ganancia_acumulada))\n",
        "print(\"Ratio dias con rent positivas(%):  \",\"{:.2f}\".format(Ratio_operaciones_positivas))\n",
        "print(\"Beneficio por compra:             \", \"{:.2f}\".format(Beneficio_x_compra))\n",
        "print(\"Beneficio total del periodo:       \",\"{:.0f}\".format(Beneficio_x_compra*Compras ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c1ZK-8lWZEf"
      },
      "source": [
        "# plot it\n",
        "#fig, ax = plt.subplots(1,1)\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "#ax.hist(Operaciones[\"meanLA\"], bins=10, color='lightblue',cumulative=False)\n",
        "\n",
        "#ax.hist(Operaciones[\"meanCO\"], bins=10, color='darkblue',cumulative=False)\n",
        "ax.hist(Operaciones[\"ganancia\"], bins=100, color='darkblue',cumulative=False)\n",
        "#ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "#ax.xaxis.set_major_formatter(mdates.DateFormatter('%d.%m.%y'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQmX044rWHmO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "matplotlib.pyplot.grid(b=True)\n",
        "#plt.hist(data[\"mean5\"],20)\n",
        "#plt.hist(data[\"mean34\"],20)\n",
        "plt.scatter(Operaciones[\"meanLA\"],Operaciones[\"meanCO\"],c=Operaciones[\"resultado +\"])\n",
        "#plt.scatter(Operaciones[\"meanLA\"],Operaciones[\"meanCO\"], c=Operaciones[\"ganancia\"],cmap=ListedColormap(colors))\n",
        "plt.xlabel(r'Posteriori', fontsize=15)\n",
        "plt.ylabel(r'meanCO', fontsize=15)\n",
        "plt.title('Probabilidades')\n",
        "plt.axis([0, 1, 0, 1])\n",
        "\n",
        "\n",
        "x = np.linspace(0, 1, 100)\n",
        "y = np.linspace(0, 1, 100,axis=0)\n",
        "#Generamos una grafica lineal para una recta en X\n",
        "plt.plot(x, (4/35)*x, label='compra 1')\n",
        "plt.plot(x, (40/11)*x-5.5/11.0, label='compra 2')\n",
        "plt.plot(x, (40/11)*x-13/11.0, label='venta 1')\n",
        "plt.plot(x, (4/35)*x+3/35, label='venta 1')\n",
        "plt.plot(x, 0.40/x, label='venta 3')\n",
        "plt.plot(x, 1-0.025/(0.9*x**2), label='compra 3')\n",
        "\n",
        "#Agregamos las etiquetas y añadimos una leyenda.\n",
        "#plt.xlabel('Eje X')\n",
        "#plt.ylabel('Eje Y')\n",
        "#plt.title(\"Simple Plot\")\n",
        "#plt.legend()\n",
        "plt.savefig('grafica_lineal.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajlSd-obRxiu"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "plt.plot(Operaciones[\"fecha\"],Operaciones[\"inversion_acum\"])\n",
        "plt.plot(Operaciones[\"fecha\"],Operaciones[\"ganancia_acum\"]*5000)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wka2VYV3AUHY"
      },
      "source": [
        "start=\"2010-01-01\"\n",
        "end=\"2020-12-31\"\n",
        "dataset=pd.DataFrame()\n",
        "for ticker in ibex:\n",
        "  data_ticker1=Betas(ticker,start=start,end=end).betas()\n",
        "  data_ticker=data_ticker1.iloc[-25:].copy()\n",
        "  dataset=dataset.append(data_ticker,ignore_index=True)\n",
        "\n",
        "#mercados=[igbm,ibex,cac,dax,mib,dow,nasdaq,s_and_p_100]\n",
        "mercados=[ibex,cac,dax,mib,dow]\n",
        "for mercado in mercados:\n",
        "  vender=[]\n",
        "  previo=[]\n",
        "  mantener_posicion=[]\n",
        "  comprar=[]\n",
        "  for ticker in mercado:\n",
        "    datas=Betas(ticker,start=start,end=end).betas()\n",
        "    F=datas.index[-1]\n",
        "    ultimo=datas.loc[F, [\"meanLA\"]]    \n",
        "    ult=np.asarray(ultimo)\n",
        "    ultimop=datas.loc[F, [\"meanCO\"]]\n",
        "    ultp=np.asarray(ultimop)\n",
        "\n",
        "    if ult==2:\n",
        "      comprar.append(ticker)\n",
        "\n",
        "    elif ultp==2:\n",
        "      previo.append(ticker)\n",
        "\n",
        "    elif ult==0:\n",
        "      mantener_posicion.append(ticker)\n",
        "    elif ult==1:\n",
        "      vender.append(ticker)\n",
        "    else:\n",
        "      mantener_posicion.append(ticker)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1d8ol0sDgt8"
      },
      "source": [
        "start=\"2020-01-01\"\n",
        "end=\"2020-12-31\"\n",
        "dataset=pd.DataFrame()\n",
        "for ticker in ibex:\n",
        "  data_ticker1=Betas(ticker,start=start,end=end).betas()\n",
        "  data_ticker=data_ticker1.iloc[-1:].copy()\n",
        "  data_ticker[\"Ticker\"]=ticker\n",
        "  obs=eval_env.reset()[3][0][0].numpy()\n",
        "  ret=eval_env.reset()[1][0].numpy()\n",
        "  action,EC= best_policy(obs,0)  \n",
        "  print(\"Ticker\",data_ticker.loc[-1:,\"Ticker\"],\"Accion\",action)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}